{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cfca86-031d-4233-a803-47b4b6295117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google.generativeai in c:\\anaconda3\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\anaconda3\\lib\\site-packages (from google.generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\anaconda3\\lib\\site-packages (from google-api-core->google.generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\anaconda3\\lib\\site-packages (from google-api-core->google.generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google.generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\anaconda3\\lib\\site-packages (from google-api-python-client->google.generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\anaconda3\\lib\\site-packages (from google-api-python-client->google.generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\anaconda3\\lib\\site-packages (from google-api-python-client->google.generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\anaconda3\\lib\\site-packages (from pydantic->google.generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\anaconda3\\lib\\site-packages (from pydantic->google.generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\anaconda3\\lib\\site-packages (from pydantic->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63b3040-f344-436d-83fd-247583be879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d8220f-1f24-44ba-99b9-ef0366320d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatSession',\n",
       " 'GenerationConfig',\n",
       " 'GenerativeModel',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'annotations',\n",
       " 'caching',\n",
       " 'configure',\n",
       " 'create_tuned_model',\n",
       " 'delete_file',\n",
       " 'delete_tuned_model',\n",
       " 'embed_content',\n",
       " 'embed_content_async',\n",
       " 'get_base_model',\n",
       " 'get_file',\n",
       " 'get_model',\n",
       " 'get_operation',\n",
       " 'get_tuned_model',\n",
       " 'list_files',\n",
       " 'list_models',\n",
       " 'list_operations',\n",
       " 'list_tuned_models',\n",
       " 'protos',\n",
       " 'responder',\n",
       " 'string_utils',\n",
       " 'types',\n",
       " 'update_tuned_model',\n",
       " 'upload_file',\n",
       " 'utils']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c806b0b-ae4e-4e7d-9bbc-8ba49665c71b",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "\n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\generativeai\\models.py:202\u001b[39m, in \u001b[36mlist_models\u001b[39m\u001b[34m(page_size, client, request_options)\u001b[39m\n\u001b[32m    199\u001b[39m     request_options = {}\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     client = \u001b[43mget_default_model_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m client.list_models(page_size=page_size, **request_options):\n\u001b[32m    205\u001b[39m     model = \u001b[38;5;28mtype\u001b[39m(model).to_dict(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:372\u001b[39m, in \u001b[36mget_default_model_client\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_default_model_client\u001b[39m() -> glm.ModelServiceAsyncClient:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_default_client\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:289\u001b[39m, in \u001b[36m_ClientManager.get_default_client\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    287\u001b[39m client = \u001b[38;5;28mself\u001b[39m.clients.get(name)\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m.clients[name] = client\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:249\u001b[39m, in \u001b[36m_ClientManager.make_client\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ga_exceptions.DefaultCredentialsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    243\u001b[39m     e.args = (\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  No API_KEY or ADC found. Please either:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Set the `GOOGLE_API_KEY` environment variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    248\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_metadata:\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:241\u001b[39m, in \u001b[36m_ClientManager.make_client\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m patch_colab_gce_credentials():\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         client = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ga_exceptions.DefaultCredentialsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    243\u001b[39m     e.args = (\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  No API_KEY or ADC found. Please either:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Set the `GOOGLE_API_KEY` environment variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    248\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\client.py:661\u001b[39m, in \u001b[36mModelServiceClient.__init__\u001b[39m\u001b[34m(self, credentials, transport, client_options, client_info)\u001b[39m\n\u001b[32m    653\u001b[39m     transport_init: Union[\n\u001b[32m    654\u001b[39m         Type[ModelServiceTransport], Callable[..., ModelServiceTransport]\n\u001b[32m    655\u001b[39m     ] = (\n\u001b[32m   (...)\u001b[39m\u001b[32m    658\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[..., ModelServiceTransport], transport)\n\u001b[32m    659\u001b[39m     )\n\u001b[32m    660\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport = \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33masync\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._transport):\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER.isEnabledFor(\n\u001b[32m    675\u001b[39m         std_logging.DEBUG\n\u001b[32m    676\u001b[39m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\transports\\grpc.py:239\u001b[39m, in \u001b[36mModelServiceGrpcTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[39m\n\u001b[32m    234\u001b[39m             \u001b[38;5;28mself\u001b[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(\n\u001b[32m    235\u001b[39m                 certificate_chain=cert, private_key=key\n\u001b[32m    236\u001b[39m             )\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grpc_channel:\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[32m    252\u001b[39m     channel_init = channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).create_channel\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\transports\\base.py:103\u001b[39m, in \u001b[36mModelServiceTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m     credentials, _ = google.auth.load_credentials_from_file(\n\u001b[32m    100\u001b[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ignore_credentials:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     credentials, _ = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mwith_gdch_audience\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\Lib\\site-packages\\google\\auth\\_default.py:739\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    731\u001b[39m             _LOGGER.warning(\n\u001b[32m    732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    735\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    736\u001b[39m             )\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: \n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information."
     ]
    }
   ],
   "source": [
    "list(genai.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a05c0eb-5bb7-478c-8703-a9adac05b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyA-5eVFIzm-vROcAQIaIXNbG-rD1S6w2jU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9581b3bd-172b-43d6-9dd9-7835a3fc6bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash Latest',\n",
       "       display_name='Gemini Flash Latest',\n",
       "       description='Latest release of Gemini Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-lite-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash-Lite Latest',\n",
       "       display_name='Gemini Flash-Lite Latest',\n",
       "       description='Latest release of Gemini Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Pro Latest',\n",
       "       display_name='Gemini Pro Latest',\n",
       "       description='Latest release of Gemini Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash-Lite',\n",
       "       description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-image-preview',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Nano Banana',\n",
       "       description='Gemini 2.5 Flash Preview Image',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-image',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Nano Banana',\n",
       "       description='Gemini 2.5 Flash Preview Image',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Preview 09-2025',\n",
       "       display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       description='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-09-25',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
       "       description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3-pro-preview-11-2025',\n",
       "       display_name='Gemini 3 Pro Preview',\n",
       "       description='Gemini 3 Pro Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-image-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/nano-banana-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-robotics-er-1.5-preview',\n",
       "       base_model_id='',\n",
       "       version='1.5-preview',\n",
       "       display_name='Gemini Robotics-ER 1.5 Preview',\n",
       "       description='Gemini Robotics-ER 1.5 Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       description='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/deep-research-pro-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='deepthink-exp-05-20',\n",
       "       display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
       "       description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Ultra',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Fast',\n",
       "       description='Vertex served Imagen 4.0 Fast model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description='Veo 3',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3 fast',\n",
       "       description='Veo 3 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1',\n",
       "       description='Veo 3.1',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-fast-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1 fast',\n",
       "       description='Veo 3.1 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Native Audio Latest',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Latest',\n",
       "       description='Latest release of Gemini 2.5 Flash Native Audio',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='12-2025',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(genai.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f692c473-e121-435a-885c-b1b2babd8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for i in genai.list_models():\n",
    "  print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae45999-686c-4e4d-8c65-12e8c19daa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea837b20-d2c6-444e-a9a9-30b0f2f272e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_async_client',\n",
       " '_client',\n",
       " '_generation_config',\n",
       " '_get_tools_lib',\n",
       " '_model_name',\n",
       " '_prepare_request',\n",
       " '_safety_settings',\n",
       " '_system_instruction',\n",
       " '_tool_config',\n",
       " '_tools',\n",
       " 'cached_content',\n",
       " 'count_tokens',\n",
       " 'count_tokens_async',\n",
       " 'from_cached_content',\n",
       " 'generate_content',\n",
       " 'generate_content_async',\n",
       " 'model_name',\n",
       " 'start_chat']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ac3771-73d7-4777-8bf8-18a167406f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model.generate_content(\"tell me which Ai is trending in all over world now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1c93b7-7c57-46ab-aab7-0aa180272767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While it's a rapidly evolving field, and what's \"trending\" can shift quickly, the AI models and concepts that are currently dominating global conversations and adoption are primarily:\n",
      "\n",
      "1.  **OpenAI's ChatGPT (and underlying GPT models like GPT-4o):**\n",
      "    *   **Why it's trending:** Still the most recognized and widely used conversational AI. Its recent release of **GPT-4o** (the \"o\" stands for \"omni\") brought significant advancements in speed, cost-efficiency, and especially **multimodality**  allowing for much more natural and expressive voice conversations, real-time understanding of video/images, and generation across text, audio, and visual. This release captured massive global attention.\n",
      "\n",
      "2.  **Google's Gemini (and its various versions like Gemini Advanced):**\n",
      "    *   **Why it's trending:** Google's direct and powerful competitor to OpenAI. Gemini represents Google's full-stack AI capabilities, integrated into many of its products (Search, Workspace, Android). Google is heavily pushing Gemini's **multimodal capabilities** and its ability to reason across different types of data, directly challenging GPT's dominance.\n",
      "\n",
      "3.  **Anthropic's Claude (especially Claude 3 Opus, Sonnet, Haiku):**\n",
      "    *   **Why it's trending:** While perhaps less known to the general public than ChatGPT or Gemini, Claude has gained significant traction, especially in enterprise and professional settings. Its **long context windows** (ability to process vast amounts of text) and strong emphasis on **AI safety and ethics** make it a preferred choice for many businesses and developers. Claude 3 models are highly competitive in performance.\n",
      "\n",
      "**Beyond specific models, these broader AI trends are globally dominant:**\n",
      "\n",
      "*   **Multimodality:** The ability of AI to understand, process, and generate content across different modalities  text, image, audio, video  in a unified way. GPT-4o and Gemini are leading this charge.\n",
      "*   **AI Assistants/Copilots:** Integrating AI directly into everyday tools and operating systems (e.g., Microsoft Copilot in Windows, Office, Edge; Google Gemini in Workspace and Android). This is making AI more accessible and practical for a wider audience.\n",
      "*   **Open-Source AI Models (e.g., Meta's Llama series):** While not a consumer-facing product in the same way, the rapid development and adoption of open-source large language models (LLMs) are fueling innovation and allowing countless startups and researchers to build their own AI applications, making this a huge \"behind-the-scenes\" trend.\n",
      "*   **Generative AI for Media (Image/Video Generation):** Models like Midjourney (for images), Stable Diffusion (open-source image generation), and especially **OpenAI's Sora** (for remarkably realistic video generation, though not yet public) continue to captivate and demonstrate the creative power of AI.\n",
      "\n",
      "In summary, if you have to pick one, **OpenAI's ChatGPT (with its GPT-4o release)** is arguably the most broadly trending and talked-about AI right now due to its direct impact on user interaction and the showcasing of advanced multimodal capabilities. However, Google's Gemini is a very close second, and the overall push towards **multimodal AI assistants** is the overarching global trend.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3831b829-f559-43f9-8887-bdb55dfd07df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-genai<2.0.0,>=1.53.0 (from langchain-google-genai)\n",
      "  Downloading google_genai-1.55.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.2 (from langchain-google-genai)\n",
      "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from langchain-google-genai) (2.12.4)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\anaconda3\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (9.1.2)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.53.0->langchain-google-genai)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.33)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai)\n",
      "  Downloading langsmith-0.4.59-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (6.0.3)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai)\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.0.0)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai)\n",
      "  Using cached orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_genai-1.55.0-py3-none-any.whl (703 kB)\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 703.4/703.4 kB 13.1 MB/s  0:00:00\n",
      "Downloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
      "Downloading langsmith-0.4.59-py3-none-any.whl (413 kB)\n",
      "Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Using cached orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Installing collected packages: filetype, websockets, uuid-utils, orjson, langsmith, langchain-core, google-genai, langchain-google-genai\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [websockets]\n",
      "   ----- ---------------------------------- 1/8 [websockets]\n",
      "   ----- ---------------------------------- 1/8 [websockets]\n",
      "   -------------------- ------------------- 4/8 [langsmith]\n",
      "   -------------------- ------------------- 4/8 [langsmith]\n",
      "   -------------------- ------------------- 4/8 [langsmith]\n",
      "   -------------------- ------------------- 4/8 [langsmith]\n",
      "   -------------------- ------------------- 4/8 [langsmith]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------- -------------- 5/8 [langchain-core]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ------------------------------ --------- 6/8 [google-genai]\n",
      "   ----------------------------------- ---- 7/8 [langchain-google-genai]\n",
      "   ---------------------------------------- 8/8 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 google-genai-1.55.0 langchain-core-1.2.0 langchain-google-genai-4.0.0 langsmith-0.4.59 orjson-3.11.5 uuid-utils-0.12.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566a53c7-3706-48a7-887d-8abbefa55ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68aeea09-e171-4d62-a1a1-492d180d5ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGoogleGenerativeAI',\n",
       " 'ComputerUse',\n",
       " 'Environment',\n",
       " 'GoogleGenerativeAI',\n",
       " 'GoogleGenerativeAIEmbeddings',\n",
       " 'HarmBlockThreshold',\n",
       " 'HarmCategory',\n",
       " 'MediaResolution',\n",
       " 'Modality',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_common',\n",
       " '_compat',\n",
       " '_enums',\n",
       " '_function_utils',\n",
       " '_image_utils',\n",
       " 'chat_models',\n",
       " 'create_context_cache',\n",
       " 'data',\n",
       " 'embeddings',\n",
       " 'llms',\n",
       " 'utils']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain_google_genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8efa642c-de29-4a1a-aeb2-5549841b5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a tough one as \"top\" can be subjective and vary based on era, format, and personal preference. However, considering a blend of statistical achievements, impact, leadership, and overall contribution to Indian cricket across formats and eras, here's a widely accepted list of the top 5 Indian cricketers:\n",
      "\n",
      "1.  **Sachin Tendulkar:**\n",
      "    *   **Why:** Widely regarded as the \"God of Cricket.\" Most runs in both Test and ODI formats, most centuries (100 international hundreds). Played for over two decades, captivating billions and carrying the hopes of a nation. His records are unparalleled.\n",
      "\n",
      "2.  **Virat Kohli:**\n",
      "    *   **Why:** A modern legend, often called \"King Kohli.\" Incredible run-scoring ability across all formats, particularly in ODIs where he holds numerous records for fastest to various run milestones and centuries. A fierce competitor and an exceptional chaser, he redefined fitness standards in Indian cricket.\n",
      "\n",
      "3.  **MS Dhoni:**\n",
      "    *   **Why:** \"Captain Cool\" and one of the greatest finishers and captains in cricket history. He is the only captain to win all three ICC white-ball trophies (T20 World Cup 2007, ODI World Cup 2011, Champions Trophy 2013). An exceptional wicketkeeper-batsman and a tactical genius.\n",
      "\n",
      "4.  **Kapil Dev:**\n",
      "    *   **Why:** India's greatest fast-bowling all-rounder and the captain who led India to its first Cricket World Cup victory in 1983. He was the first player to achieve the double of 5,000 Test runs and 400 Test wickets, a testament to his all-round brilliance.\n",
      "\n",
      "5.  **Sunil Gavaskar:**\n",
      "    *   **Why:** \"The Little Master\" was a pioneering opener who redefined Test match batting. He was the first batsman to score 10,000 Test runs and 30 Test centuries. He faced some of the most fearsome fast bowlers in history without a helmet and set the standard for future Indian batsmen.\n",
      "\n",
      "**Honorable Mentions:** Rahul Dravid, Rohit Sharma, Sourav Ganguly, Anil Kumble, Jasprit Bumrah, Yuvraj Singh, and many others have also had phenomenal careers and immense impact. The depth of Indian cricketing talent is truly remarkable!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=\"AIzaSyA-5eVFIzm-vROcAQIaIXNbG-rD1S6w2jU\"\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"who are the top 5 indian crickets \").content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee581b-8b2a-4a0e-890d-c4d03b3d03f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
